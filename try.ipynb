{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "dim = 8\n",
    "seq_len = 100\n",
    "theta = 10000\n",
    "\n",
    "freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "# 生成 token 序列索引 t = [0, 1,..., seq_len-1]\n",
    "t = torch.arange(seq_len, device=freqs.device)\n",
    "# freqs.shape = [seq_len, dim // 2] \n",
    "freqs = torch.outer(t, freqs).float()  # 计算m * \\theta input 和 vec2 的外积。如果 input 是大小为 n 的向量，而 vec2 是大小为 m 的向量，则 out 必须是大小为 (n×m) 的矩阵。不可广播\n",
    "\n",
    "# 计算结果是个复数向量\n",
    "# 假设 freqs = [x, y]\n",
    "# 则 freqs_cis = [cos(x) + sin(x)i, cos(y) + sin(y)i]\n",
    "freqs_cis = torch.polar(torch.ones_like(freqs), freqs) # 极坐标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1e-09\n"
     ]
    }
   ],
   "source": [
    "print(-1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 1, 1, 1], [1, 0, 0, 0, 0], [1, 0, 0, 0, 0]]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m dp \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m n] \u001b[38;5;241m+\u001b[39m [[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m (n \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(dp)\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "m = 3\n",
    "n = 5\n",
    "dp = [[1] * n] + [[1] + [0] * (n - 1) for _ in range(m-1)]\n",
    "print(dp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\distributed\\distributed_c10d.py:608: UserWarning: Attempted to get default timeout for nccl backend, but NCCL support is not compiled\n",
      "  warnings.warn(\"Attempted to get default timeout for nccl backend, but NCCL support is not compiled\")\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error initializing torch.distributed using env:// rendezvous: environment variable WORLD_SIZE expected, but not set",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdist\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_process_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnccl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(dist\u001b[38;5;241m.\u001b[39mget_rank())\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\distributed\\c10d_logger.py:86\u001b[0m, in \u001b[0;36m_time_logger.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     85\u001b[0m     t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime_ns()\n\u001b[1;32m---> 86\u001b[0m     func_return \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime_ns() \u001b[38;5;241m-\u001b[39m t1\n\u001b[0;32m     89\u001b[0m     msg_dict \u001b[38;5;241m=\u001b[39m _get_msg_dict(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\distributed\\distributed_c10d.py:1177\u001b[0m, in \u001b[0;36minit_process_group\u001b[1;34m(backend, init_method, timeout, world_size, rank, store, group_name, pg_options)\u001b[0m\n\u001b[0;32m   1173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m store \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1174\u001b[0m     rendezvous_iterator \u001b[38;5;241m=\u001b[39m rendezvous(\n\u001b[0;32m   1175\u001b[0m         init_method, rank, world_size, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m   1176\u001b[0m     )\n\u001b[1;32m-> 1177\u001b[0m     store, rank, world_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrendezvous_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1178\u001b[0m     store\u001b[38;5;241m.\u001b[39mset_timeout(timeout)\n\u001b[0;32m   1180\u001b[0m     \u001b[38;5;66;03m# Use a PrefixStore to avoid accidental overrides of keys used by\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m     \u001b[38;5;66;03m# different systems (e.g. RPC) in case the store is multi-tenant.\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\distributed\\rendezvous.py:239\u001b[0m, in \u001b[0;36m_env_rendezvous_handler\u001b[1;34m(url, timeout, **kwargs)\u001b[0m\n\u001b[0;32m    237\u001b[0m     world_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(query_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworld_size\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 239\u001b[0m     world_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[43m_get_env_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWORLD_SIZE\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    242\u001b[0m master_addr \u001b[38;5;241m=\u001b[39m _get_env_or_raise(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMASTER_ADDR\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    243\u001b[0m master_port \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(_get_env_or_raise(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMASTER_PORT\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\distributed\\rendezvous.py:219\u001b[0m, in \u001b[0;36m_env_rendezvous_handler.<locals>._get_env_or_raise\u001b[1;34m(env_var)\u001b[0m\n\u001b[0;32m    217\u001b[0m env_val \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(env_var, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m env_val:\n\u001b[1;32m--> 219\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _env_error(env_var)\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_val\n",
      "\u001b[1;31mValueError\u001b[0m: Error initializing torch.distributed using env:// rendezvous: environment variable WORLD_SIZE expected, but not set"
     ]
    }
   ],
   "source": [
    "import torch.distributed as dist\n",
    "import os\n",
    "\n",
    "dist.init_process_group(backend='nccl', rank=0)\n",
    "print(dist.get_rank())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 4])\n",
      "(tensor([[0.2000, 0.9000],\n",
      "        [0.5000, 0.2000],\n",
      "        [0.1000, 0.2000],\n",
      "        [0.3000, 0.5000],\n",
      "        [0.7000, 0.6000],\n",
      "        [0.6000, 0.1000],\n",
      "        [0.7000, 0.7000],\n",
      "        [0.8000, 0.4000],\n",
      "        [0.8000, 1.0000],\n",
      "        [0.7000, 0.9000]], grad_fn=<SplitWithSizesBackward0>), tensor([[0.7000, 0.7000],\n",
      "        [0.6000, 0.0000],\n",
      "        [0.8000, 0.8000],\n",
      "        [0.8000, 1.0000],\n",
      "        [0.8000, 0.2000],\n",
      "        [0.2000, 0.2000],\n",
      "        [0.2000, 0.7000],\n",
      "        [0.5000, 0.6000],\n",
      "        [0.1000, 0.3000],\n",
      "        [0.9000, 0.9000]], grad_fn=<SplitWithSizesBackward0>))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "w_values = [\n",
    "        [0.2, 0.9, 0.7, 0.7],\n",
    "        [0.5, 0.2, 0.6, 0.0],\n",
    "        [0.1, 0.2, 0.8, 0.8],\n",
    "        [0.3, 0.5, 0.8, 1.0],\n",
    "        [0.7, 0.6, 0.8, 0.2],\n",
    "        [0.6, 0.1, 0.2, 0.2],\n",
    "        [0.7, 0.7, 0.2, 0.7],\n",
    "        [0.8, 0.4, 0.5, 0.6],\n",
    "        [0.8, 1.0, 0.1, 0.3],\n",
    "        [0.7, 0.9, 0.9, 0.9],\n",
    "    ]\n",
    "w = torch.tensor(w_values, requires_grad=True)\n",
    "\n",
    "print(w.shape)\n",
    "\n",
    "sub_w = torch.split(w, [2, 2], dim=1)\n",
    "\n",
    "print(sub_w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9883, 0.5427, 0.8878, 0.8828, 0.7226, 0.6405],\n",
      "        [0.9407, 0.0168, 0.6099, 0.6709, 0.5025, 0.3553],\n",
      "        [0.8024, 0.0245, 0.2826, 0.6800, 0.9657, 0.4073],\n",
      "        [0.3605, 0.6188, 0.5342, 0.1723, 0.7519, 0.0761]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(4, 6)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\datasets\\load.py:1486: FutureWarning: The repository for tatoeba contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/tatoeba\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'translation'],\n",
      "        num_rows: 164106\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "dataset = datasets.load_dataset(\"tatoeba\", lang1=\"en\", lang2=\"he\")\n",
    "print(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
