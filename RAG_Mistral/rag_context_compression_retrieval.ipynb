{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceHub\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PyPDFLoader\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Chroma\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langchain'"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "# from dotenv import load_dotenv\n",
    "import os\n",
    "from getpass import getpass\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = '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'#getpass(\"Enter HuggingFace Hub Token:\")\n",
    "\n",
    "loader = PyPDFLoader(\"/content/CommonInsuranceTerms.pdf\")\n",
    "documents = loader.load()\n",
    "print(len(documents))\n",
    "print(documents[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=700,chunk_overlap=70)\n",
    "split_documents = text_splitter.split_documents(documents)\n",
    "print(len(split_documents))\n",
    "print(split_documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = SentenceTransformerEmbeddings(model_name=\"llmware/industry-bert-insurance-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id =\"llmware/bling-sheared-llama-1.3b-0.1\"\n",
    "llm = HuggingFaceHub(repo_id=repo_id,\n",
    "                     model_kwargs={\"temperature\":0.3,\"max_length\":500})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_docs(docs):\n",
    "  print(f\"\\n{'-'* 100}\\n\".join([F\"Document{i+1}:\\n\\n\" + d.page_content for i,d in enumerate(docs)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(split_documents,\n",
    "                                    embeddings,\n",
    "                                    collection_metadata={\"hnsw:space\":\"cosine\"},\n",
    "                                    persist_directory=\"/content/stores/insurance\")\n",
    "vectorstore.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\":2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.get_relevant_documents(query=\"What is Group life insurance?\")\n",
    "pretty_print_docs(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "#making the compressor\n",
    "compressor = LLMChainExtractor.from_llm(llm=llm)\n",
    "#compressor retriver = base retriever + compressor\n",
    "compression_retriever = ContextualCompressionRetriever(base_retriever=retriever,\n",
    "                                                       base_compressor=compressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 默认压缩器提示\n",
    "print(compressor.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用LLMChainFilter选择要传递给LLM的查询\n",
    "from getpass import getpass\n",
    "import os\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
    "#\n",
    "os.environ[\"OPENAI_API_KEY \"] = getpass()\n",
    "#\n",
    "embdeddings_filter = EmbeddingsFilter(embeddings=embeddings)\n",
    "compression_retriever_filter = ContextualCompressionRetriever(base_retriever=retriever,\n",
    "                                                       base_compressor=embdeddings_filter)\n",
    "#\n",
    "compressed_docs = compression_retriever_filter.get_relevant_documents(query=\"What is Group Life Insurance?\")\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用RetrievalQA链实现问答功能\n",
    "from langchain.chains import RetrievalQA\n",
    "qa = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                 chain_type=\"stuff\",\n",
    "                                 retriever=compression_retriever_filter,\n",
    "                                 verbose=True)\n",
    "#Ask Question\n",
    "qa(\"What is Coinsurance?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline\n",
    "将压缩器和文档转换器串在一起\n",
    "embeddings:langchain_core.embeddings.Embeddings[必需]用于嵌入文档内容和查询的嵌入\n",
    "k：可选[int]=20要返回的相关文档数。可以设置为“无”，在这种情况下，必须指定similarity_threshold。默认值为20。\n",
    "similarity_fn:Callable=用于比较文档的相似性函数。函数期望将两个矩阵（List[List[foat]]）作为输入，并返回一个分数矩阵，其中值越高表示相似性越大。\n",
    "similarity_threshold:可选[foat]=无用于确定两个文档何时相似到足以被视为冗余的阈值。如果k设置为“无”，则必须指定默认值“无”。       \n",
    "在这里，我们创建了一个由冗余过滤器+相关过滤器组成的管道，其中冗余过滤器过滤掉重复的上下文，相关过滤器仅提取相关上下文。\n",
    "EmbeddingsRedundantFilter：我们可以识别类似的文档并过滤掉冗余；\n",
    "EmbeddingsFilter：通过嵌入文档和查询并只返回那些与查询具有足够相似嵌入的文档，提供了一种更便宜、更快的选择。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.document_transformers import EmbeddingsRedundantFilter\n",
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "#\n",
    "redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings)\n",
    "relevant_filter = EmbeddingsFilter(embeddings=embeddings,k=5)\n",
    "#making the pipeline\n",
    "pipeline_compressor = DocumentCompressorPipeline(transformers=[redundant_filter,relevant_filter])\n",
    "# compressor retriever\n",
    "compression_retriever_pipeline = ContextualCompressionRetriever(base_retriever=retriever,\n",
    "                                                       base_compressor=pipeline_compressor)\n",
    "## print the prompt\n",
    "print(compression_retriever_pipeline)\n",
    "## Get relevant documents\n",
    "compressed_docs = compression_retriever_pipeline.get_relevant_documents(query=\"What is Coinsurance?\")\n",
    "pretty_print_docs(compressed_docs)\n",
    "\n",
    "compressed_docs = compression_retriever_pipeline.get_relevant_documents(query=\"What is Earned premium?\")\n",
    "pretty_print_docs(compressed_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用llmware/bling-sheared-llama-1.3b-0.1模型实现问答功能\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "template =\"\"\"\n",
    "<human>:\n",
    "Context:{context}\n",
    "​\n",
    "Question:{question}\n",
    "​\n",
    "Use the above Context to answer the user's question.Consider only the Context provided above to formulate response.If the Question asked does not match with the Context provided just say 'I do not know thw answer'.\n",
    "<bot>:\n",
    "​\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(input_variables=[\"context\",\"question\"],template=template)\n",
    "chain_type_kwargs = {\"prompt\":prompt}\n",
    "print(prompt)\n",
    "####\n",
    "from langchain.chains import RetrievalQA\n",
    "qa = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                 chain_type=\"stuff\",\n",
    "                                 retriever=compression_retriever_pipeline,\n",
    "                                 chain_type_kwargs=chain_type_kwargs,\n",
    "                                 return_source_documents=True,\n",
    "                                 verbose=True)\n",
    "#\n",
    "qa(\"What is Group Insurance Policy?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = qa(\"What is Long-term care benefits?\")\n",
    "print(response['result'].split(\"<|endoftext|>\")[0])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建新管道\n",
    "# 新的pipeline = 压缩器 + 冗余过滤器 + 相关过滤器\n",
    "# compressor:LLMChainExtractor ,将迭代最初返回的文档，并从每个文档中仅提取与查询相关的内容\n",
    "compressor = LLMChainExtractor.from_llm(llm=OpenAI(temperature=0.3,openai_api_key=api_key))\n",
    "#\n",
    "new_pipeline = DocumentCompressorPipeline(transformers=[compressor,redundant_filter,relevant_filter])\n",
    "new_compression_retriever = ContextualCompressionRetriever(base_retriever=retriever,\n",
    "                                                       base_compressor=new_pipeline)\n",
    "compressed_docs = new_compression_retriever.get_relevant_documents(query=\"What is Coinsurance?\")\n",
    "pretty_print_docs(compressed_docs)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现问答链\n",
    "from langchain.chains import RetrievalQA\n",
    "qa = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                 chain_type=\"stuff\",\n",
    "                                 retriever=new_compression_retriever,\n",
    "                                 chain_type_kwargs=chain_type_kwargs,\n",
    "                                 return_source_documents=True,\n",
    "                                 verbose=True)\n",
    "#\n",
    "response = qa(\"What is Coinsurance?\")\n",
    "print(response['result'].split(\"<|endoftext|>\")[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dpo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
